{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "# from PIL import Image, ImageDraw\n",
    "#import PIL\n",
    "from PIL import Image as PILImage\n",
    "from PIL import ImageDraw as PILImageDraw\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "from IPython.display import clear_output, display \n",
    "from IPython.display import Image as IPImage\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "\n",
    "from model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Running on device: {cuda_device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detect_model = MTCNN(keep_all=True, device=cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotion recognition\n",
    "def load_trained_model(model_path):\n",
    "    model = Face_Emotion_CNN()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_trained_model('../models/emotion_m2pt.pt')\n",
    "emotion_dict = {0: 'Neutral', 1: 'Happy', 2: 'Surprise', 3: 'Sad',\n",
    "                    4: 'Angry', 5: 'Disgust', 6: 'Fear'}\n",
    "\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "        transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "/build/opencv-XDqSFW/opencv-3.2.0+dfsg/modules/imgproc/src/imgwarp.cpp:3492: error: (-215) ssize.width > 0 && ssize.height > 0 in function resize\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b59775ea87ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mresize_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_frame\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /build/opencv-XDqSFW/opencv-3.2.0+dfsg/modules/imgproc/src/imgwarp.cpp:3492: error: (-215) ssize.width > 0 && ssize.height > 0 in function resize\n"
     ]
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "frame_count = 0\n",
    "\n",
    "\n",
    "def get_name(face_img):\n",
    "    return 'josh'\n",
    "\n",
    "def get_emotion(face_img):\n",
    "    return 'happy'\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    while(True):\n",
    "        frame_count += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        _, frame = video_capture.read()\n",
    "        faces, _ = face_detect_model.detect(frame)\n",
    "        \n",
    "        if faces is not None:\n",
    "            \n",
    "            \n",
    "            for top, left, bottom, right  in faces:\n",
    "                \n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "                resize_frame = cv2.resize(gray[int(top):int(bottom), int(left):int(right)], (48, 48))\n",
    "\n",
    "                X = resize_frame/256\n",
    "                X = PILImage.fromarray((X))\n",
    "                X = val_transform(X).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    log_ps = model.cpu()(X)\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    pred = emotion_dict[int(top_class.numpy())] \n",
    "        \n",
    "                face_img = frame[0:300, 0:200]\n",
    "                cv2.rectangle(frame, (top, left), (bottom, right), (0, 100, 0), 2)\n",
    "\n",
    "\n",
    "                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "                x, y, w, h = int(top+2), int(right-50), int(left-2), int(bottom-26)\n",
    "\n",
    "                scale = 0.045 # this value can be from 0 to 1 (0,1] to change the size of the text relative to the image\n",
    "                fontScale = min(w,h)/(25/scale)\n",
    "\n",
    "                cv2.putText(frame, \"Name\", ((int(top)+3), (int(right)-18)), font, fontScale, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "                cv2.putText(frame, pred, ((int(top)+3), (int(right)-4)), font, fontScale, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "               \n",
    "   \n",
    "        ret, img = cv2.imencode('.jpg', frame)\n",
    "        img = IPImage(data=img)\n",
    "        display(img)\n",
    "        \n",
    "        # Display the resulting image\n",
    "        #cv2.imshow('Video', frame)\n",
    "\n",
    "        # Hit 'q' on the keyboard to quit!\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "finally:\n",
    "    video_capture.release()\n",
    "  \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
