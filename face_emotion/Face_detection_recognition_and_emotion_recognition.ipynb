{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "#import torchvision.transforms as transforms\n",
    "from PIL import Image as PILImage\n",
    "from PIL import ImageDraw as PILImageDraw\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "from IPython.display import clear_output, display\n",
    "from IPython.display import Image as IPImage\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "from torch import nn, optim, as_tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.init import *\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "from emotion_models import *\n",
    "from face_encodings import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute on Jetson Xavier NX CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Running on device: {cuda_device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Recognition transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model requires two custom classes to load. \n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "class normalize(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(normalize, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Face Recognition transfer learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load our transfer trained model (with our custom classes for classmates)\n",
    "model_face_recog = torch.load('../models/face_recognition_transf.pt')\n",
    "model_face_recog.to(cuda_device)\n",
    "model_face_recog.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we load the data_transforms and class_names specified in the facerec_transfer_training notebook. \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        #Note: Our images must be 160x160 for model (Training & Val). \n",
    "        transforms.Resize(size = (160,160)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(size = (160,160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "data_dir = '../data/face/images.m2'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                              batch_size=8, \n",
    "                                             shuffle=True)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_name(face_img):\n",
    "    #face_img = face_img / 256\n",
    "    face_img = PILImage.fromarray((face_img))\n",
    "    val_transform = data_transforms['val']\n",
    "    face_img = val_transform(face_img).unsqueeze(0)\n",
    "    with torch.no_grad(): \n",
    "        model_face_recog.eval()\n",
    "        face_img = face_img.to(cuda_device)\n",
    "        output = model_face_recog(face_img)\n",
    "        top_p, pred = output.topk(1,dim =1)\n",
    "        pred = pred.cpu()\n",
    "        name = class_names[int(pred.numpy())]\n",
    "        return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion detection using Kaggle dataset with best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {0: 'Neutral', 1: 'Happy', 2: 'Surprise', 3: 'Sad',\n",
    "                    4: 'Angry'}\n",
    "    \n",
    "def load_trained_model(model_path):\n",
    "    model = to_device(ResNet(1, len(emotion_dict)), device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage), strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FER_image(img_path, model):\n",
    "\n",
    "\n",
    "    #emotion_dict = {0: 'Neutral', 1: 'Happy', 2: 'Surprise', 3: 'Sad', 4: 'Angry'}\n",
    "    \n",
    "    val_transform = tt.Compose([\n",
    "        tt.ToTensor()])\n",
    "\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier('../models/haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(img)\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "        resize_frame = cv2.resize(gray[y:y + h, x:x + w], (48, 48))\n",
    "        X = resize_frame/256\n",
    "        X = PILImage.fromarray((resize_frame))\n",
    "        X = val_transform(X).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            log_ps = model.cpu()(X)\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            pred = emotion_dict[int(top_class.numpy())]\n",
    "        cv2.putText(img, pred, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emotion_m1 = load_trained_model('../models/emotion_m1tr-0723_64e_.63.pth')\n",
    "FER_image('../data/test_images/test.jpg', model_emotion_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FER_image('../data/test_images/angry_2.jpg', model_emotion_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FER_image('../data/test_images/happy.jpeg', model_emotion_m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion detection using Pre-trained model with best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotion recognition\n",
    "def load_trained_model(model_path):\n",
    "    model = Face_Emotion_CNN()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_emotion_m2 = load_trained_model('../models/emotion_m2pt.pt')\n",
    "emotion_dict = {0: 'Neutral', 1: 'Happy', 2: 'Surprise', 3: 'Sad',\n",
    "                    4: 'Angry', 5: 'Disgust', 6: 'Fear'}\n",
    "\n",
    "\n",
    "FER_image('../data/test_images/angry_2.jpg', model_emotion_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection\n",
    "#### Load Face Detection Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_face_detect = MTCNN(keep_all=True, device=cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run models with live Video feed on Nvidia Jetson Xavier NX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "frame_count = 0\n",
    "\n",
    "val_transform = tt.Compose([\n",
    "        tt.ToTensor()])\n",
    "    \n",
    "try:\n",
    "    while(True):\n",
    "        frame_count += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        _, frame = video_capture.read()\n",
    "        faces, _ = model_face_detect.detect(frame)\n",
    "        \n",
    "        if faces is not None:\n",
    "            \n",
    "            \n",
    "            for top, left, bottom, right  in faces:\n",
    "                \n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "                resize_frame = cv2.resize(gray[int(top):int(bottom), int(left):int(right)], (48, 48))\n",
    "\n",
    "                X = resize_frame/256\n",
    "                X = PILImage.fromarray((X))\n",
    "                X = val_transform(X).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    model_emotion_m1.eval()\n",
    "                    log_ps = model_emotion_m1.cpu()(X)\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    pred = emotion_dict[int(top_class.numpy())] \n",
    "        \n",
    "                #face_img = frame[0:300, 0:200]\n",
    "                face_img = frame[int(top):int(bottom), int(left):int(right)]\n",
    "                cv2.rectangle(frame, (top, left), (bottom, right), (0, 100, 0), 2)\n",
    "                \n",
    "                #Face name recognition\n",
    "                rgb_face = face_img[:, :, ::-1]\n",
    "                face_name = get_face_name(rgb_face)\n",
    "\n",
    "                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "                x, y, w, h = int(top+2), int(right-50), int(left-2), int(bottom-26)\n",
    "\n",
    "                scale = 0.045 # this value can be from 0 to 1 (0,1] to change the size of the text relative to the image\n",
    "                fontScale = min(w,h)/(25/scale)\n",
    "\n",
    "                cv2.putText(frame, face_name, ((int(top)+3), (int(right)-18)), font, fontScale, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "                cv2.putText(frame, pred, ((int(top)+3), (int(right)-4)), font, fontScale, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "               \n",
    "   \n",
    "        ret, img = cv2.imencode('.jpg', frame)\n",
    "        img = IPImage(data=img)\n",
    "        display(img)\n",
    "        \n",
    "        # Display the resulting image\n",
    "        #cv2.imshow('Video', frame)\n",
    "\n",
    "        # Hit 'q' on the keyboard to quit!\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    video_capture.release()\n",
    "  \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "#Emotion recognition\n",
    "def load_trained_model(model_path):\n",
    "    model = Face_Emotion_CNN()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_trained_model('../models/emotion_m2pt.pt')\n",
    "emotion_dict = {0: 'Neutral', 1: 'Happy', 2: 'Surprise', 3: 'Sad',\n",
    "                    4: 'Angry', 5: 'Disgust', 6: 'Fear'}\n",
    "\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "        transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    process_this_frame = True\n",
    "        \n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # # If a match was found in known_face_encodings, just use the first one.\n",
    "            # if True in matches:\n",
    "            #     first_match_index = matches.index(True)\n",
    "            #     name = known_face_names[first_match_index]\n",
    "\n",
    "            # Or instead, use the known face with the smallest distance to the new face\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "        #resize_frame = cv2.resize(gray[y:y + h, x:x + w], (48, 48))\n",
    "        resize_frame = cv2.resize(gray[top:bottom, left:right], (48, 48))\n",
    "\n",
    "        X = resize_frame/256\n",
    "        X = Image.fromarray((X))\n",
    "        X = val_transform(X).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            log_ps = model.cpu()(X)\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            pred = emotion_dict[int(top_class.numpy())] \n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 100, 0), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        #cv2.rectangle(frame, (left+2, bottom - 50), (right-2, bottom-26), (152, 251, 152), cv2.FILLED)\n",
    "        #font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        x, y, w, h = left+2, bottom-50, right-2, bottom-26\n",
    "        sub_img = frame[y:h, x:w]\n",
    "        white_rect = np.ones(sub_img.shape, dtype=np.uint8) * 255\n",
    "        res = cv2.addWeighted(sub_img, 0.8, white_rect, 0.1, 1.0)\n",
    "        frame[y:h, x:w] = res\n",
    "\n",
    "        scale = 0.05 # this value can be from 0 to 1 (0,1] to change the size of the text relative to the image\n",
    "        fontScale = min(w,h)/(25/scale)\n",
    "\n",
    "        #cv2.putText(frame, name, (left + 6, bottom - 29), font, 1.0, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 29), font, fontScale, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        #cv2.rectangle(frame, (left+2, bottom - 25), (right-2, bottom-2), (47, 79, 79), cv2.FILLED)\n",
    "        #font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        #cv2.putText(frame, pred, (left + 6, bottom - 4), font, 1.0, (34, 139, 34), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        # First we crop the sub-rect from the image\n",
    "        x, y, w, h = left+2, bottom-25, right-2, bottom-2\n",
    "        #sub_img = frame[y:y+h, x:x+w]\n",
    "        sub_img = frame[y:h, x:w]\n",
    "        white_rect = np.ones(sub_img.shape, dtype=np.uint8) * 255\n",
    "        res = cv2.addWeighted(sub_img, 0.6, white_rect, 0.2, 1.0)\n",
    "\n",
    "        # Putting the image back to its position\n",
    "        #frame[y:y+h, x:x+w] = res\n",
    "        frame[y:h, x:w] = res\n",
    "        #cv2.putText(frame, pred, (left + 6, bottom - 4), font, fontScale, (34, 139, 34), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, pred, (left + 6, bottom - 4), font, fontScale, (255, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        ##  Draw a label with a name below the face\n",
    "        #cv2.rectangle(frame, (left+2, bottom - 35), (right-2, byttom-2), (152, 251, 152), cv2.FILLED)\n",
    "        #font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        #font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        #cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (34, 139, 34), 2, cv2.LINE_AA)\n",
    "\n",
    "    #ret, img = cv2.imencode('.jpg', frame)\n",
    "    #img = IPImage(data=img)\n",
    "    #display(img)\n",
    "    \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
